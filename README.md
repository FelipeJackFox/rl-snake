# ğŸ RL Snake with PPO + Optuna

Este proyecto implementa el clÃ¡sico juego de Snake entrenado con **Reinforcement Learning (RL)** usando **PPO (Proximal Policy Optimization)** y bÃºsqueda de hiperparÃ¡metros con **Optuna**.

ğŸš€ Entrenamiento en paralelo, bÃºsqueda de hiperparÃ¡metros y soporte para visualizar el progreso con TensorBoard.

---

## ğŸ“‚ Estructura del proyecto
rl-snake/
agents/ # Scripts de entrenamiento y evaluaciÃ³n (PPO)

â”‚â”€â”€ envs/ # Entorno Snake en Gymnasium + Pygame

â”‚â”€â”€ search/ # BÃºsqueda de hiperparÃ¡metros con Optuna

â”‚â”€â”€ utils/ # Callbacks, wrappers, helpers

â”‚â”€â”€ models/ # Modelos guardados (ej. final_model.zip)

â”‚â”€â”€ requirements.txt # Dependencias

â”‚â”€â”€ README.md # Este archivo

---

## âš™ï¸ InstalaciÃ³n

Clona el repo y crea un entorno virtual:

```bash
git clone https://github.com/<TU_USUARIO>/rl-snake.git
cd rl-snake
python -m venv .venv
.venv\Scripts\activate   # Windows
# source .venv/bin/activate  # Linux/Mac

pip install -r requirements.txt
```
## ğŸƒâ€â™‚ï¸ Entrenamiento
ğŸ” BÃºsqueda de hiperparÃ¡metros con Optuna

Ejemplo de bÃºsqueda en paralelo (3 terminales corriendo a la vez):

```bash
python -m search.optuna_search --n_trials 40 --timesteps 400000 --storage sqlite:///optuna.db --study_name snake_search --wide --best_out models/best_params.json
```

ğŸ¤– Entrenamiento robusto con los mejores parÃ¡metros

```bash
python -m agents.ppo_train_use_best --params models/best_params.json --timesteps 20000000 --n_envs 8 --grid_size 12 --max_no_food_steps 80 --tb runs --models models
```

ğŸ“Š VisualizaciÃ³n en TensorBoard

```bash
Durante o despuÃ©s del entrenamiento:
```

## ğŸ® EvaluaciÃ³n
Jugar con el mejor modelo

```bash
python -m agents.eval_play --model models/best_model.zip --fps 30
```

Jugar con el modelo final

```bash
python -m agents.eval_play --model models/final_model.zi --fps 30
```
